# RL Configuration for Waypoint Navigation Task

rl:
  algorithm: PPO
  policy: MlpPolicy
  
  # PPO Hyperparameters
  learning_rate: 0.0003
  n_steps: 2048  # Steps per rollout
  batch_size: 64
  n_epochs: 10
  gamma: 0.99  # Discount factor
  gae_lambda: 0.95
  clip_range: 0.2
  clip_range_vf: null
  ent_coef: 0.01  # Entropy coefficient
  vf_coef: 0.5  # Value function coefficient
  max_grad_norm: 0.5
  
  # Policy network architecture
  policy_kwargs:
    net_arch: [256, 256]
    activation_fn: relu

environment:
  task: waypoint
  obs_type: kin
  act_type: rpm
  domain_randomization: true
  num_envs: 8  # Parallel environments
  
  # Task-specific settings
  num_waypoints: 4
  waypoint_spacing: 1.0  # meters
  waypoint_tolerance: 0.1
  max_episode_steps: 1000

domain_randomization:
  enabled: true
  
  # Physics randomization
  mass_range: [0.8, 1.2]  # Multiplier
  inertia_range: [0.8, 1.2]
  gravity_range: [0.95, 1.05]
  
  # Disturbances
  motor_noise: 0.05  # Percentage
  wind_disturbance: 0.1  # Max force (N)
  wind_gust_prob: 0.1  # Probability per step
  
  # Delays
  action_delay: [0, 0.05]  # seconds
  observation_delay: [0, 0.02]
  
  # Sensor noise
  position_noise: 0.01  # meters
  velocity_noise: 0.02  # m/s
  orientation_noise: 0.01  # radians

training:
  total_timesteps: 500000
  save_freq: 10000
  eval_freq: 5000
  n_eval_episodes: 20
  
  # Early stopping
  early_stopping: true
  patience: 10  # Eval iterations without improvement
  min_improvement: 0.01
  
  # Checkpointing
  save_path: models/ppo/
  save_replay_buffer: false

reward:
  # Reward weights
  position_weight: 1.0
  velocity_weight: 0.1
  action_weight: 0.01
  
  # Penalties
  crash_penalty: -10.0
  timeout_penalty: -5.0
  
  # Success bonus
  waypoint_bonus: 10.0
  completion_bonus: 50.0

logging:
  tensorboard: true
  log_dir: logs/tensorboard/
  verbose: 1
  
  # WandB (optional)
  use_wandb: false
  wandb_project: drone-hybrid-rl
  wandb_entity: null
