# Hybrid Controller Configuration for Trajectory Tracking

hybrid:
  # Hybrid mode: 'adaptive', 'switching', 'weighted'
  mode: adaptive
  
  # Weighting (for weighted mode)
  pid_weight: 0.3
  rl_weight: 0.7
  
  # Switching (for switching mode)
  switching_threshold: 0.5  # Error threshold for switching
  
  # Adaptation (for adaptive mode)
  adaptation_rate: 0.01
  min_weight: 0.1
  max_weight: 0.9

pid:
  # Load pre-trained PID
  load_path: models/ppo/best_hover.pkl
  
  # PID parameters (if training from scratch)
  kp: [0.4, 0.4, 1.0]
  ki: [0.0, 0.0, 0.1]
  kd: [0.2, 0.2, 0.5]

rl:
  algorithm: PPO
  policy: MlpPolicy
  
  # Load pre-trained RL policy (optional)
  load_path: models/ppo/best_waypoint.zip
  
  # PPO Hyperparameters (for fine-tuning)
  learning_rate: 0.0001  # Lower for fine-tuning
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.005  # Lower entropy for stability

task:
  name: trajectory
  
  # Trajectory type: 'circle', 'figure_eight', 'spiral', 'custom'
  trajectory_type: figure_eight
  
  # Trajectory parameters
  radius: 2.0  # meters
  height: 1.5  # meters
  speed: 0.5  # m/s
  duration: 30.0  # seconds
  
  # Tracking settings
  position_tolerance: 0.15  # meters
  velocity_tolerance: 0.2  # m/s
  max_episode_steps: 1500

environment:
  obs_type: kin
  act_type: rpm
  domain_randomization: true
  num_envs: 4

domain_randomization:
  enabled: true
  mass_range: [0.8, 1.2]
  inertia_range: [0.8, 1.2]
  motor_noise: 0.05
  wind_disturbance: 0.15  # Stronger for trajectory
  action_delay: [0, 0.05]
  sensor_noise: 0.02

training:
  total_timesteps: 300000
  save_freq: 10000
  eval_freq: 5000
  n_eval_episodes: 20
  save_path: models/hybrid/

reward:
  # Trajectory tracking rewards
  position_error_weight: 2.0
  velocity_error_weight: 0.5
  orientation_weight: 0.2
  
  # Control effort
  action_smoothness: 0.05
  energy_efficiency: 0.01
  
  # PID-RL cooperation
  cooperation_bonus: 1.0  # Reward agreement between controllers
  conflict_penalty: -0.5  # Penalize disagreement
  
  # Task completion
  trajectory_completion_bonus: 100.0

hybrid_training:
  # Train hybrid system in stages
  stage_1:
    name: pid_warmup
    timesteps: 50000
    pid_weight: 0.9
    rl_weight: 0.1
  
  stage_2:
    name: balanced
    timesteps: 150000
    pid_weight: 0.5
    rl_weight: 0.5
  
  stage_3:
    name: rl_emphasis
    timesteps: 100000
    pid_weight: 0.3
    rl_weight: 0.7

evaluation:
  trajectories:
    - circle
    - figure_eight
    - spiral
    - lemniscate
  
  metrics:
    - position_rmse
    - velocity_rmse
    - tracking_error
    - energy_consumption
    - success_rate

logging:
  tensorboard: true
  log_dir: logs/tensorboard/
  save_trajectories: true
  trajectory_dir: results/videos/
  verbose: 1
